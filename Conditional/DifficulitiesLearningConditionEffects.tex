\providecommand{\master}{..}
\documentclass[\master/Master.tex]{subfiles}
\begin{document}

\texttt{Thomas: } Note that this section is missing a proper introduction. We are considering actions containing one or more conditional effects (called conditionals in the rest of this section). We use them in the form 
\begin{equation*}
    \forall x, y, z, \dots \left[ q(x, y) \right] \quad when \quad 
        \left[ p(z, x) \land f(y) \land \dots \right]
\end{equation*}

To keep it simple, we only support effects which are conjunctions of conditionals of the above form.

We do not support actions with parameters.


\begin{definition}
	An explicit state is a set of predicates where the negative predicates are explicitly mentioned. To avoid generating the infinite set, it is limited by a set of predicate names $P$ and a set of objects $\objs$:
	\begin{equation*}
		\ts\left(S, \preds, \objs\right) = S \cup
		\left\{ \neg p \mid 
		p\left(o_1, \dots, o_{|p|}\right) \notin S \land 
		\left\{o_1, \dots, o_{|p|} \right\} \in \objs \land
		p \in \preds
		\right\}
	\end{equation*}
	The arguments $P$ and $\mathcal{O}$ are normally left out when their values are clear from the context.
\end{definition}


In STRIPS-Style actions the objects $A(O_1,O_2,\ldots,O_{n-1},O_n)$ are the only objects affected by the action.
Thus any predicate that contains objects other than $O_1,O_2,\ldots,O_{n-1},O_n$ can be discarded as irrelevant when learning either the effects or the preconditions.
However when actions contain conditional effects with support for use of quantifiers this claim no longer holds.
To properly discuss the subject we introduce \glits, which is the set of all grounded literals.
\begin{definition}
	All possible grounded literals \glits is defined as:
	\begin{equation*}
		\glits =
		\left\{ \neg p\left(O_1, \dots, O_{|p|}\right), 
				p\left(O_1, \dots, O_{|p|}\right) 
				\mid 
		p \in \preds \land 
		\left\{O_1, \dots, O_{|p|} \right\} \subseteq \objs
		\right\}
	\end{equation*}
	
	to avoid defining it as a function assume $\preds$ and $\objs$ are given from the context.
\end{definition}

A problem occurs when the set of objects \objs is infinite (or intractably large), which causes the set $\glits$ to also be infinite.
Meaning it is theoretically possible to have chain of connected atoms, such as 
\begin{equation*}
	p(v_1,v_2), p(v_2,v_3),\dots, p(v_{|\objs|-2},v_{|\objs|-1}), p(v_{|\objs|-1},v_{|\objs|})
\end{equation*}
 as a precondition to a conditional.
This means that initially we cannot assume that $|\varC| < |\objs|$.

However if the environment has a finite number of objects, then we can limit $\glits$ to a finite set.
This is important as we now have an initial assumption of possible literals for both the preconditions and the effects.

%\subsubsection{Limitations}
In order to simplify the problem we have made the following restrictions on conditional effects we can learn:

\newlist{propenum}{enumerate}{1} % also creates a counter called 'propenumi'
\setlist[propenum]{label=R. \arabic*}
\crefalias{propenumi}{arestriction} 

\begin{arestriction}\leavevmode
\begin{propenum}[label=R.\arabic*]
	\item Actions cannot have variables. i.e. all actions are of form $A()$, 
	we have added this restricting to study the problem in its most minimal form.	
		
	\item The environment must be fully observable.
	\item The environment must contain a finite number of objects.
	\item \label{rst:ca:no-disjuntive-conditionals} No disjunctive preconditions or effects. As a consequence of this restriction, no two conditionals can contain the same effect if their preconditions differ.
	\item \label{rst:ca:no-multiple-effect} No multiples of the same effect.

		  I.e.\ the effect $\forall_x p(x) \land q(x)$ is supported, but $\forall_{x, y} p(x, y) \land p(x, x) $ is not.

	\item \label{rst:ca:no-disjoint-preconditions} No disjoint preconditions.

		  A disjoint precondition is when it is not connected to an effect or another non-disjoint precondition through its variables.
		  I.e.\ If a conditional effect had the effect $p(x)$ a disjoint precondition could be $p(y)$ as $x \neq y$ however if it also had a precondition $q(x,y)$ this would connect the two variables thus making $p(y)$ non-disjoint.
\end{propenum}
\end{arestriction}
%
%Another aspect of conditional effects is the notion of multiple different effects in the same action. This is supported given that it does not break restriction~\ref{rst:ca:no-disjuntive-conditionals}; which it does if two conditionals have the same effect, as that is equivalent to an disjunctive precondition.

In this section we will show how to approach the concept of learning conditional effect. We will show what problems that occur using initial models and what changes to the models are necessary. Lastly we will introduce a new way to model atoms by using hypergraphs.


Given the restriction we presented we are able to simplify the problem. That is, we can model every possible action schema where each conditional only contains one effect.

\begin{example}
	Take for instance the conditional:	
	\begin{align*}
		&A():&  \\
		&\quad
		\forall_{x, y}
			\left[
				\texttt{q}(x) \land \texttt{h}(y)
			\right]
		\; when \;
		\left[ \texttt{p}(x,y) \right]
	\end{align*}		
	That conditional is equivalent to: 
	\begin{align*}
		A():&  \\
			 &\quad\forall_{x, y}
				\left[
				\texttt{h}(y)
				\right]
				\; when \;
				\left[ \texttt{p}(x,y) \right] \\
		&\quad\land 	\\		
		&\quad\forall_{x, y}
		\left[
		\texttt{q}(x)
		\right]
		\; when \;
		\left[ \texttt{p}(x,y) \right]
	\end{align*}
	
\end{example}



\begin{proposition}[Single effect per conditional]\label{prop:ca:singleConditional}
    Consider a state transition $\left(S, a, S'\right)$ and a grounded literal $p \in \geffects$. By \Cref{rst:ca:no-disjuntive-conditionals} and \Cref{rst:ca:no-multiple-effect}, 
    
    there is exactly one conditional in $A$ that could be responsible for producing $p$. Consequently, any two literals in $\geffects$ with the same predicate are effects of the same conditional.
\end{proposition}

\begin{figure}
	\centering{%
	\includegraphics[width=1\textwidth]{\master/Graphics/house_conditional_knowledge.png}}
	\caption{\label{fig:ca:house-example}Three states of light problem for \Cref{ex:ca:light-on}}
\end{figure}

\begin{example}\label{ex:ca:light-on}
Consider the problem of turning on light as shown in \Cref{fig:ca:house-example}.
In the figure there are three different cases of a person learning how to turn on the
light. In the first two cases the agent successfully turns on the light while in the last nothing occurs.

In the first case we learn that to turn on the light there are these possible precondition:
\begin{itemize}
	\item There must be a sofa in the north-east corner.
	\item There must be a lamp in the north-west corner. With an agent on the same spot.
	\item There must be a table in the middle.
\end{itemize}
Even though we intuitively know that it is only the lamp which is the only required object in this room for the light to turn on, we cannot discard any of the other objects present as not being a precondition.
\end{example}

\subsection{Preconditions from states}
Objects are defined by the atoms in the state which they satisfy. As such when we say that all objects are assumed to be possible preconditions for specific effects to occur, what we are actually saying is that a specific composition of predicates is the requirement for the effects to occur.
This means if we interpret the objects as variables connecting the predicates in a pattern, then we can get an initial set of preconditions by using the state $S$ which led to the effects. In real terms interpreting objects as variables means mapping each object 
to a to different variable.
\begin{equation}
	\sigma(P,\delta) =  \leftarrow \left\{p\left(\delta(O_1),\ldots,\delta(O_n) \right) \mid p(O_1,\ldots,O_n) \in P  \right\}
\end{equation}
For instance for the state.
$\{ p(O_1), p(O_2),\ldots,p(O_n)\}$ using a mapping
\begin{equation*}
\delta = \{O_1 \mapsto v_1, O_2 \mapsto v_2 \ldots, O_n \mapsto v_n\}
\end{equation*}
where $(v_1, v_2,\dots,v_{n-1},v_n)$ are variables for the quantifier of the conditional effect; Interpreting this state's objects as variables means to apply this mapping to the state's objects
\begin{equation*}
\{ p(\delta(O_1)), p(\delta(O_2)),\ldots,p(\delta(O_n))\} = \{ p(v_1), p(v_2),\ldots,p(v_n)\}
\end{equation*}
To see how it works in sokoban see \Cref{ex:ca:sokoban-moveleft-action}

Another property we can take advantage of is that similar to non-conditional preconditions, if an effect is successfully added to the state then that disproves all preconditions not currently satisfied.
This gives us the following:

\begin{theorem}\label{thm:ca:precondition-state}
If $\gamma (S,a) = S'$ then $\pre(e) \subseteq \sigma(\ts(S))$  for the effect $e \in \geffects$.
Therefore limiting the preconditions to literals in $\ts(S)$ is guaranteed not to remove actual preconditions.

\begin{proof}[\textbf{Proof by contradiction}] Lets assume that there are more preconditions than present in $\ts(S)$ for the effects \geffects.
	In this case the effects would not occur as their required preconditions are missing, thus the preconditions must be satisfied in $\ts(S)$.
	As such we can conclude $\ts(S)$ contains at least all precondition atoms.
\end{proof}
\end{theorem}


\begin{figure}
    \hspace*{0.1\textwidth}%
    \begin{subfigure}{0.35\textwidth}
        \centering
        \resizebox{\linewidth}{!}{\input{\master/Graphics/moveLeftBefore.pgf}}
        \caption{Before execution of \texttt{MoveLeft}}
    \end{subfigure}%
    \hspace*{0.1\textwidth}%
    \begin{subfigure}{0.35\textwidth}
        \centering
        \resizebox{\linewidth}{!}{\input{\master/Graphics/moveLeftAfter.pgf}}
        \caption{After execution of \texttt{MoveLeft}}
    \end{subfigure}
    \hspace*{0.1\textwidth}
	\caption{\label{fig:ca:sokoban-moveleft-action}\texttt{MoveLeft} action being used in a sokoban world for \Cref{ex:ca:sokoban-moveleft-action} }

\end{figure}

\begin{example}\label{ex:ca:sokoban-moveleft-action}
    Let us consider an example of acquiring preconditions from the state. In \Cref{fig:ca:sokoban-moveleft-action} we see an example of an agent in the sokoban world pushing a box using a MoveLeft action with zero parameters.

    The action schema that produced the effect is:
    \begin{align*}
    &MoveLeft():&  \\
    &\quad
        \forall_{x, y}
            \left[
                \neg\texttt{sokobanAt}(y) \land \texttt{sokobanAt}(x)
            \right]
            \quad when \quad
            \left[ \texttt{adj-h}(x,y) \land \texttt{sokobanAt}(y) \right]& \\
    &\quad
        \forall_{x, y, z, b}
            \left[ \neg\texttt{at}(b,y) \land \texttt{at}(b,x) \right]
            \quad when \quad
            \left[
                \begin{gathered}
                     \texttt{adj-h}(x,y) \land \texttt{adj-h}(y,z) \land \\
                          \texttt{sokobanAt}(z) \land \texttt{at}(b, y)
                \end{gathered}
            \right] &
    \end{align*}

    The state transition that occurred:

    \begin{equation*}\label{eq:s_before}
    S_{before} =
        \left\{
            \begin{gathered}
                \texttt{adj-h}(t1, t2), \texttt{adj-h}(t2, t3), \\
                \texttt{at}(box,t2), \texttt{sokobanAt}(t3)
            \end{gathered}
        \right\}
    \end{equation*}
    \begin{equation*}
    S_{after} =
        \left\{
            \begin{gathered}
                \texttt{adj-h}(t1, t2), \texttt{adj-h}(t2, t3), \\
                \texttt{at}(box,t1), \texttt{sokobanAt}(t2)
            \end{gathered}
        \right\}
    \end{equation*}
    The effects of the state transition we calculate:
    \begin{equation*}
    \Delta S =
        \left\{
            \begin{gathered}
                \texttt{at}(box, t1), \texttt{sokobanAt}(t2), \\
                \neg\texttt{at}(box,t2), \neg\texttt{sokobanAt}(t3)
            \end{gathered}
        \right\}
    \end{equation*}
    To interpret the objects as variables we provide some mapping of objects to variables, such a mapping could be:
    \begin{equation*}
    \delta =
        \left [
            \begin{gathered}
                t1 \mapsto x \\
                t2 \mapsto y \\
                t3 \mapsto z \\
                box \mapsto b
            \end{gathered}
        \right ]
    \end{equation*}
    We apply the mapping to the objects of $S_{before}$:
    \begin{equation*}
        \sigma(S_{before}, \delta) =
        \begin{gathered}
            \left\{
                \begin{gathered}
                    \texttt{adj-h}(\delta (t1), \delta (t2)), \\
                    \texttt{adj-h}(\delta (t2), \delta (t3)), \\
                    \texttt{at}(\delta (box),\delta (t2)), \\
                     \texttt{sokobanAt}(\delta (t3))
                \end{gathered}
            \right\}
            =
            \left\{
                \begin{gathered}
                    \texttt{adj-h}(x, y), \\
                     \texttt{adj-h}(y, z), \\
                    \texttt{at}(y), \\
                    \texttt{sokobanAt}(z)
                \end{gathered}
            \right\}
        \end{gathered}
    \end{equation*}
    We also apply the mapping to the effects $\Delta S$
    \begin{equation*}
        \sigma(\geffects, \delta) =
            \left\{
                \begin{gathered}
                    \texttt{at}(\delta (box),\delta (t1)), \\
                    \texttt{sokobanAt}(\delta (t2)), \\
                    \neg\texttt{at}(\delta (box),\delta (t2)), \\
                     \neg\texttt{sokobanAt}(\delta (t3))
                \end{gathered}
            \right\}
            =
            \left\{
                \begin{gathered}
                    \texttt{at}(b,x),
                    \texttt{sokobanAt}(y), \\
                    \neg\texttt{at}(b,y),
                    \neg\texttt{sokobanAt}(z)
                \end{gathered}
            \right\}
    \end{equation*}

    The resulting literals of $S_{before}$ and $\geffects$ are the preconditions and the effects.
    Therefore our hypothesis of the action scheme is:
    \begin{align*}
    &MoveLeft_{hyp}():&  \\
    &\quad
        \forall_{x, y, z, b}
            \left[
            \begin{gathered}
                \texttt{at}(b, x) \land \texttt{sokobanAt}(y) \land \\ \neg\texttt{at}(b,y) \land \neg\texttt{sokobanAt}(z) \quad
            \end{gathered}
            \right]
        when
            \left[
            \begin{gathered}
            \quad \texttt{adj-h}(x, y) \land \texttt{adj-h}(y, z) \land \\ \texttt{at}(b,y) \land \texttt{sokobanAt}(z)
            \end{gathered}
            \right]&
    \end{align*}

    According to \Cref{thm:ca:precondition-state} we can use $MoveLeft_{hyp}$ on $S_{before}$ and get $S_{after}$.
    \begin{align*}
    &\gamma(S_{before},MoveLeft_{hyp}) =
        \left\{
            \begin{gathered}
                \texttt{adj-h}(t1, t2), \texttt{adj-h}(t2, t3), \\
                \texttt{at}(box,t1), \texttt{sokobanAt}(t2)
            \end{gathered}
        \right\}
        = S_{after}
    &
    \end{align*}

    We observe that this is indeed correct.
\end{example}

\subsection{Initial approach to reducing preconditions}

As different scenarios are explored the unproven set $\Up_p$ of preconditions can be reduced.

\begin{definition}
The unproven set $Up_p$ is the preconditions which has neither been proven nor disproven, initially it contains all  literals $\glits$.
\end{definition}

We will now consider how this unproven set can be constructed using the methods for proving and disproving preconditions presented in \Cref{sec:NC:Preconditions}, show why it is insufficient and introduce a new model which fits the domain better.

By \Cref{rst:ca:no-disjuntive-conditionals} and \Cref{rst:ca:no-multiple-effect}, each literal $p \in \geffects$ is spawned by exactly one conditional. As such, it can be deduced that if a predicate $p\left(u_1, \dots, u_{|p|}\right)$ (where $\left\{u_1, \dots, u_{|p|}\right\} \subseteq \objs$) occurs in $\geffects$ after application of $a$, then $A$ contains a conditional of the form

\begin{equation*}
    \forall x_1, \dots, x_n 
        \left[ p\left(\delta\left(u_1\right), \dots, \delta \left(u_{|p|}\right) \right) \right] \quad when \quad 
        \left[ \left<unknown\right> \right]
\end{equation*}
where $\left\{\delta \left( u_1 \right), \dots, \delta \left(u_{|p|} \right) \right\} \subseteq \left\{ x_1, \dots, x_n \right\} \subseteq \delta [\mathcal{O}]$ and $\delta$ is a substitution function as explained above. Observe that once this conditional has been proven to exist, the problem of finding its preconditions is similar to finding the preconditions of a non-conditional action schema $A'$ accepting as many parameters as the number of objects in the world, ie. 

\begin{align*}
    \begin{split}
        \textsc{Action} &\; \textit{A'}\left(\delta \left( o_1 \right), \ldots, \delta \left( o_{|\mathcal{O}|}\right)\right): \\
        \textsc{Pre}: \; & \left< unknown \right> \\
        \textsc{Eff}: \; & p\left( \delta \left(u_1\right), \dots, \delta \left( u_{|p|} \right)\right)
    \end{split}
\end{align*}

in which case the grounding function becomes $\delta^{-1}$. Thus, the function $d(s)$ from \Cref{sec:NC:Preconditions} can be used to disprove preconditions, and the set of predicates neither proven nor disproven to be preconditions becomes $\mathbb{CP} \setminus d(s)$. Note that since $\mathbb{P}_{A'} = \mathbb{CP}_A$, $|d(s)| = |\mathbb{P}_{A'}|$, as each predicate is either present or absent from the state. 

As all predicates in $\Delta s$ with the same name are effects of the same conditional, it is tempting to consider them different applications of $A'$ and prove knowledge about their preconditions as in \Cref{sec:NC:Preconditions}. However, the above only holds for a single grounded predicate observed to be the effect of the conditional; if a predicate with the same name but different arguments occurs in $\Delta s$, then it is the effect of another interpretation of the universal quantifier in the conditional, and the nonconditional action schema constructed for it will not equal $A'$.

Consequently, the preconditions disproven by $d(s)$ for $A'$ are only applicable to the grounded predicate $A'$ was constructed for. To find the preconditions of the conditional, a method of combining the different disproven predicates is needed.

It follows from the above that if we have the sets of unproven preconditions $U_p1$ and $U_p1$ from two different state transitions, we can compare them and remove discrepancies by intersection:

\begin{equation}
\label{eq:unknownpredcondset}
	U_p' = U_p1 \cap U_p2
\end{equation}

This will work for removing entire predicates and only if the two sets use exact same naming for variables. Herein lies the problem of reducing preconditions; unlike non-conditional effects where entire predicates are removed at a time, for conditional effects a reduction in the preconditions can simply mean the removal of a binding between two variables.

\begin{definition}[Binding]\label{def:ca:binding}
    A binding between variables, means that variables between predicates are the same.
	Bindings that contain only one variable are free and means that no binding exists.
	It is important to note that a predicate with multiple variables can have bindings with itself.
	For instance $q(x)$ and $p(x,y)$ has a binding on their first argument as they both refer to the same variable, and $y$ is a free variable as no other predicates with $y$ exists.
\end{definition}
Thus this approach to modelling the unknown preconditions, which we used for non-conditional actions is incorrect.

\begin{example}\label{ex:ca:light-on-2}
    Following from \Cref{ex:ca:light-on} we come to the second case in \Cref{fig:ca:house-example}
    we see that we can turn on the light again but this time the sofa and the lamp has changed location and
    there is no table. From this we expand our knowledge by learning:
    \begin{itemize}
        \item Sofa and lamp location does not matter
        \item A table is not needed
    \end{itemize}
    Its important to note that since we have not seen an absence of a sofa we cannot rule out that the presence of a sofa is a precondition for the light to turn on. This is important because it shows us that by trying different scenarios we reduce the incorrect preconditions.
\end{example}

\begin{example}\label{ex:ca:nonbinding-intersection-model}
    To elucidate the problem of bindings further, imagine that we have two different $U_p$
    \begin{align*}
        U_p1 &= \{ \texttt{p}(x, x, x) \} \\
        U_p2 &= \{ \texttt{p}(x, x, y) \}
    \end{align*}
    We see that $U_p1$ has a stronger precondition, meaning that if $U_p1$ holds then so does $U_p2$ but not oppositely. I.e.\ 
    \begin{equation*}
    \forall_{x} \texttt{p}(x, x, x) \nvDash \forall_{x, y} \texttt{p}(x, x, y)  \quad and \quad
     \forall_{x, y} \texttt{p}(x, x, y) \vDash \forall_{x} \texttt{p}(x, x, x) 
    \end{equation*}
    And if we were to take the intersection between the two sets then the resulting set would be the empty set which is incorrect. As they have the similarity of having the same binding between the first and second argument. What we actually need is to have a set of bindings and take the intersection of that.
\end{example}

\subsection{A new model: Connecting paths}
From the discussion above, it is evident that the model used for preconditions of non-conditional actions is not sufficient for our needs. Specifically, it does not allow us to easily combine knowledge obtained about the same conditional from different interpretations. 
The problem, as shown above, is that this approach compares sets of predicates to prove and disprove preconditions, and determines equality of the predicates based on variable naming. However, the variable names are arbitrary and only interesting in that they describe how predicates in a set are connected to each other, since it is this connectivity that determines precondtitions.

In the following, we will explain a model based solely on connectivity between predicates. To begin with, we will explore how predicates can be connected:

\begin{definition}[Binding connection]\label{def:ca:bindingConn}
    A binding connection between indices $i$ and $j$ of two predicates $p$ and $q$ means that the variable or object at the $i$'th argument of $p$ is the same as that at the $j$'th argument of $q$. In the following, the shorthand $p_i \bc q_j$ is used to denote binding connections.
\end{definition}

\begin{definition}[Predicate connection]\label{def:ca:predicateConn}
    Similarly, the notation $p_1 \pc p_2$ denotes the rather obvious fact that the first and second argument of $p$ are both part of the same predicate $p$.
\end{definition}

Although \Cref{def:ca:predicateConn} seems insignificant, it allows us to define transitive connectivity: If $p_i \pc p_j$ and $p_j \bc q_k$ then $p_i$ and $q_k$ are connected through $p_j$. This connectivity is called a \emph{connecting path} and is denoted $p_i \pc p_j \bc q_k$.

\begin{definition}[Connecting path]\label{def:ca:connectingPath}
    A connecting path is a sequence of alternating binding and predicate connections, where any two adjacent connections shares a component, i.e.\
    \begin{equation*}
        \left< 
            \left( f_i \bc p_j \right), 
            \left( p_j \pc p_k \right), 
            \left( p_k \bc q_l \right) 
        \right>
    \end{equation*}
    where the following is used as a shorthand:
    \begin{equation*}
        f_i \bc p_j \pc p_k \bc q_l
    \end{equation*}
\end{definition}

Given a state transition $\left(S, a, S'\right)$, it now holds that for any effect $p \in \Delta S$ there exists one or more connecting paths between any argument of $p$ and any argument of any predicate $q \in \ts(S)$. Any one of these paths describe a possible precondition, and the set of all paths comprises the set of unproven preconditions $U$ for $p$. 

If another effect with the same name as $p$ occurs in $\Delta S$, we can now intersect their two path sets to arrive at a correctly merged precondition set $U'$.

\begin{example}\label{ex:ca:non-binding-interesction-model-fixed}
    <Insert example here>
\end{example}

\subsection{Proving preconditions}
Up until now we have only discussed limiting the number of possible preconditions (Unknown set) , however another aspect of learning is to determine what the actual preconditions are.
For non-conditional actions proving the preconditions of an action was a matter of using failed actions to make a list of candidate predicates that through elimination eventually got down to just one predicate which is then proclaimed proven. The approach is similar for conditional actions however instead of having candidates of predicates; conditional actions must maintain a candidate set for the connecting paths instead of entire predicates, and thus is only capable of proving a single path at a time.

\begin{theorem}\label{thm:minimum-one-binding}
    If a conditional has been observed to produce an effect (in the form of a grounded predicate $p$), and failed to produce another grounded predicate, then there must exist at least one connecting path from $p$ which is a precondition for that conditional.
\end{theorem}

\begin{proof}[\textbf{Proof by contradiction}] 
    If there is less than one path in the preconditions then that means there are zero paths. By \Cref{rst:ca:no-disjoint-preconditions}, zero paths means zero precondition. If there are zero preconditions then the effect would not have failed. Therefore there must be a minimum of one path that is a precondition.
\end{proof}

Initially we have a set of paths $B_u$, which is our set of unproven preconditions for the effect of a conditional.
If we then see that an effect does not occur in the state, the paths in the prior state that did not produce that effect becomes the candidates of possible bindings $B_c$.

Because of \Cref{thm:minimum-one-binding} the following invariant holds:
\begin{equation} \label{eq:binding_invariant}
\left| \{b  \mid  b \in B_c \land b \in preconds_{paths}(A)\} \right|  \ge 1
\end{equation}


Meaning that at least one of the paths in this set is part of the paths in the preconditions for the conditional. As paths that are not contained in $B_u$ are disproven, we can filter disproven candidates by:
\begin{equation}
    B_c' = B_c \cap B_u
\end{equation}
And thus if ever $|B_c| = 1$ then we know that it is a correct binding because of our \Cref{eq:binding_invariant} and thus it is proven.

\begin{example}\label{ex:ca:light-on-3}
    Consider the last case in \Cref{fig:ca:house-example}. This case is identical to the first case except it does not have a lamp, thus we learn that:
    \begin{itemize}
        \item A lamp is required for the effect to occur.
    \end{itemize}
    Up until this transition we have not seen proof that the lamp was a precondition even though it would be intuitive for a human that still does not mean that we can simply assume it could be that the person's action simply creates a lamp which it then proceeds to turn on, and it was the case that no lamp was created since it was present already.
    
    Interestingly like with non-conditional preconditions we require failed actions to determine the actual preconditions but unlike non-conditional preconditions an action can both succeed and fail in the same step. For instance the light was successfully turned on for the lamp but it was unsuccessful in turning on the light for the sofa.

\end{example}

\end{document}
