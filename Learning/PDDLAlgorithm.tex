\documentclass[../Master.tex]{subfiles}
\begin{document}

In the above description of the sceitific method and the algorithm derived from it, we 
We will now present a formalization of \Cref{algo:science}

Given a PDDL problem, it is then the agent's task to reach the goal specified by the problem by learning the effects and preconditions of the relevant actions. Thus, in this instance the \textit{question} from algorithm~\ref{algo:science} is a PDDL problem, and the $canAnswer$ function consists of checking whether the problem has been solved in the given state (denoted $isSolved$).

The most basic form of knowledge relevant to the PDDL agent is information about which literals are in its effects and preconditions. From this limited information, the agent can employ a strategy to construct hypothetical action specifications, constituting a hypothetical domain. For example, if a literal $p$ have neither been proven nor disproven to be a precondition of action $A$, a pessimistic strategy will include it in the hypothetical action specification for $A$, while an optimistic strategy will choose to ignore it. Both strategies will fully include proven and disproven knowledge. This hypothetical domain constitutes the \texttt{Hypothesis} from Algorithm~\ref{algo:science}.

In order to test whether the newly constructed hypothesis is sufficient for answering the question, the agent can now form a PDDL plan and test whether its execution solves the problem. Such a plan is analoguous to an \texttt{Experiment}: If every action in the plan succeeds when applied to the world, the hypothetical domain is sufficient for solving the problem from the given state. If, however, execution of an action in the plan yields a state different from what the hypothetical domain predicts, then the hypothesis is flawed. Thus, \emph{conducting} an experiment in PDDL means executing a plan, which yields a new state as well as a \texttt{Result} indicating whether it failed or succeeded, and --- in the first case --- which transition caused the failure. An algorithm for the PDDL specific \textit{conduct} function is presented in Algorithm~\ref{algo:PDDLConduct}. 

\begin{algorithm}
    \begin{algorithmic}
        \Function{$\textsc{Conduct}$} {\texttt{Plan} $l$, \texttt{World} $w$, \texttt{Domain} $d_h$}
        \State{$s \gets w.state$}
        \State{$d \gets w.domain$}
        \ForAll{grounded actions $a \in l$}
            \State{$s' \gets apply(d, s, a)$}
            \State{$s'_{h} \gets apply(d_h, s, a)$}
            \If{$s' \neq s'_h$}
                \State{\Return{$\left( \texttt{Failure} \left(s, a, s'\right), s\right)$}}
            \EndIf%
            \State{$s \gets s'$}
        \EndFor%
            \State{\Return{$\left(\texttt{Success}, s\right)$}}
        \EndFunction%
    \end{algorithmic}
    \caption{Conducting a PDDL experiment (executing a plan)}\label{algo:PDDLConduct}
\end{algorithm}

The \texttt{Result} returned by \textit{conduct} is used by the \textit{analyse} function to obtain new knowledge (as described in the following sections), which is then merged into the knowledge base.

If the generated hypothetical domain is too restrictive for the agent to find a solution to the problem (i.e.\ find a plan), no new knowledge can be gained, and the agent has the option to instead seek guidance from external sources. For this purpose,~\cite{Walsh2008} introduces the concept of a \textit{teacher}, which has full knowledge of the domain and can thus be queried for the solution to a specific problem. This solution is returned as a list of state transitions (a \texttt{Trace}), of which each one is analysed and the resulting knowledge is combined. In this context, the teacher can be seen as part of the \texttt{World}, as this already contains the actual domain. Thus, the $inquire$ function from Algorithm~\ref{algo:science} can be instantiated as querying a teacher using the $query$ function. However, other methods of inquiring knowledge from external sources could also be used.

In algorithm~\ref{algo:PDDL}, an instantiation of the more general algorithm from~\ref{algo:science} is presented. Note that both the optimistic and pessimistic algorithms from~\cite{Walsh2008}, can be derived from the one presented by applying the correct strategies and ignoring the $update$ function. 

% that since the optimistic algorithm from~\cite{Walsh2008} always finds a plan that might fail, and the pessimistic only finds successful plans or none at all, both can be derived from Algorithm~\ref{algo:PDDL} by applying the correct strategies and setting $update$ to the identity function.

\begin{algorithm}
    \begin{algorithmic}
        \Function{$\textsc{Learn-PDDL}$} {\texttt{Problem} $p$, \texttt{Strategy} $s$, \texttt{Knowledge} $k$, \texttt{World} $w$}
            \While{$\neg isSolved(p, w)$}
                \State{\texttt{Domain} $d \gets form(p, k, s)$}
                \State{\texttt{Plan} $l \gets design(w, d, s)$}
                \If{a plan is found}
                    \State{$(\texttt{Result} \, r, w) \gets conduct(l, w, d)$}
                    \State{$k \gets k + analyse(k, r)$}
                \Else%
                    \State{\texttt{Trace} $t \gets query(w, p)$}
                    \ForAll{transitions $r \in t$}
	                    \State{$k \gets k + analyse(k, r)$}
                    \EndFor%
                \EndIf%
                \State{$s \gets update(s, k)$}
            \EndWhile%
            \State{\Return{$k$}}
        \EndFunction%
    \end{algorithmic}
    \caption{Scientific learning algorithm for the PDDL framework}\label{algo:PDDL}
\end{algorithm}
\end{document}
