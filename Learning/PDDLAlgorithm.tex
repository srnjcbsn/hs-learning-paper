\documentclass[../Master.tex]{subfiles}
\begin{document}

When applied to a series of typical real-world laboratory experiments, the algorithm presented above seems rather absurd. While it is true that conducting an experiment can be seen as a procedure that changes the universe in some manner, only a small part of the universe is relevant for the result of the experiment. Additionally, the universe changes on its own, by the passing of time and other agents' actions. Thus, an abstraction is needed, which in the laboratory example would likely consist of only considering local changes, i.e.\ changes in the laboratory, petri dish, etc. 

In this section, we will show how the algorithm can be used in a universe described by logic relations and a limited set of objects. We use the PDDL framework for this, as it provides the needed abstraction. In this framework, a $\texttt{Domain}$ describes the actions available to the agent as well as the possible relations, and a \texttt{Problem} defines an intial as well as a goal state. A PDDL action contains an effect, describing the changes the action imposes on the world, and a set of preconditions, which must be satisfied by the configuration of the world for the effect to occur. A more thorough explanation of PDDL is given in Appendix~\ref{sec:app:pddl}.

% In the above description of the scientific method and the algorithm derived from it, we proposed that conducting an experiment yields an entirely new \texttt{World}. In an real-world laboratory experiment this is technically correct, as conducting the experiment changes the universe in  
%
% We will now present a formalization of \Cref{algo:science}

Given a PDDL problem, it is then the agent's task to reach the goal specified by the problem by learning the effects and preconditions of the relevant actions. Thus, in this instance the \textit{question} from algorithm~\ref{algo:science} is a PDDL problem, and the $canAnswer$ function consists of checking whether the problem has been solved in the given state (denoted $isSolved$).

The most basic form of knowledge relevant to the PDDL agent is information about which literals are in its effects and preconditions. From this limited information, the agent can employ a strategy to construct hypothetical action specifications, constituting a hypothetical domain. For example, if a literal $p$ have neither been proven nor disproven to be a precondition of action $A$, a pessimistic strategy will include it in the hypothetical action specification for $A$, while an optimistic strategy will choose to ignore it. Both strategies will fully include proven and disproven knowledge. This hypothetical domain constitutes the \texttt{Hypothesis} from Algorithm~\ref{algo:science}.

In order to test whether the newly constructed hypothesis is sufficient for answering the question, the agent can now form a PDDL plan and test whether its execution solves the problem. Such a plan is analoguous to an \texttt{Experiment}: If every action in the plan succeeds when applied to the world, the hypothetical domain is sufficient for solving the problem from the given state. If, however, execution of an action in the plan yields a state different from what the hypothetical domain predicts, then the hypothesis is flawed. Thus, \emph{conducting} an experiment in PDDL means executing a plan, which yields a new state as well as a \texttt{Result} indicating whether it failed or succeeded, and --- in the first case --- which transition caused the failure. An algorithm for the PDDL specific \textit{conduct} function is presented in Algorithm~\ref{algo:PDDLConduct}. 

\begin{algorithm}
    \begin{algorithmic}
        \Function{$\textsc{Conduct}$} {\texttt{Plan} $l$, \texttt{World} $w$, \texttt{Domain} $d_h$}
        \State{$s \gets w.state$}
        \State{$d \gets w.domain$}
        \ForAll{grounded actions $a \in l$}
            \State{$s' \gets apply(d, s, a)$}
            \State{$s'_{h} \gets apply(d_h, s, a)$}
            \If{$s' \neq s'_h$}
                \State{\Return{$\left( \texttt{Failure} \left(s, a, s'\right), s\right)$}}
            \EndIf%
            \State{$s \gets s'$}
        \EndFor%
            \State{\Return{$\left(\texttt{Success}, s\right)$}}
        \EndFunction%
    \end{algorithmic}
    \caption{Conducting a PDDL experiment (executing a plan)}\label{algo:PDDLConduct}
\end{algorithm}

The \texttt{Result} returned by \textit{conduct} is used by the \textit{analyze} function to obtain new knowledge (as described in the following sections), which is then merged into the knowledge base.

If the generated hypothetical domain is too restrictive for the agent to find a solution to the problem (i.e.\ find a plan), no new knowledge can be gained, and the agent has the option to instead seek guidance from external sources. For this purpose,~\cite{Walsh2008} introduces the concept of a \textit{teacher}, which has full knowledge of the domain and can thus be queried for the solution to a specific problem. This solution is returned as a list of state transitions (a \texttt{Trace}), of which each one is analyzed and the resulting knowledge is combined. In this context, the teacher can be seen as part of the \texttt{World}, as this already contains the actual domain. Thus, the $inquire$ function from Algorithm~\ref{algo:science} can be instantiated as querying a teacher using the $query$ function. However, other methods of inquiring knowledge from external sources could also be used.

In algorithm~\ref{algo:PDDL}, an instantiation of the more general algorithm from~\ref{algo:science} is presented. Note that both the optimistic and pessimistic algorithms from~\cite{Walsh2008}, can be derived from the one presented by applying the correct strategies and ignoring the $update$ function. 

\begin{algorithm}
    \begin{algorithmic}
        \Function{$\textsc{Learn-PDDL}$} {\texttt{Problem} $p$, \texttt{Strategy} $s$, \texttt{Knowledge} $k$, \texttt{World} $w$}
            \While{$\neg isSolved(p, w)$}
                \State{\texttt{Domain} $d \gets form(p, k, s)$}
                \State{\texttt{Plan} $l \gets design(w, d, s)$}
                \If{a plan is found}
                    \State{$(\texttt{Result} \, r, w) \gets conduct(l, w, d)$}
                    \State{$k \gets k + analyze(k, r)$}
                \Else%
                    \State{\texttt{Trace} $t \gets query(w, p)$}
                    \ForAll{transitions $r \in t$}
	                    \State{$k \gets k + analyze(k, r)$}
                    \EndFor%
                \EndIf%
                \State{$s \gets update(s, k)$}
            \EndWhile%
            \State{\Return{$k$}}
        \EndFunction%
    \end{algorithmic}
    \caption{Scientific learning algorithm for the PDDL framework}\label{algo:PDDL}
\end{algorithm}
\end{document}
