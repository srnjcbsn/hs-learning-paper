\documentclass[../Master.tex]{subfiles}
\begin{document}



In this thesis, the primary focus will be on agents learning PDDL action specifications in a static environment.

In such an environment, a number of actions that the agent can perform are available, each accepting a fixed number of objects in the world as parameters. Upon execution of an action in the world, a new world is returned, reflecting the changes made by action execution. The configuration of the world can be observed as a PDDL state. It is assumed that the mechanics of the environment can be specified as a PDDL domain, but that this specification is only partially known (or entirely unknown) to the agent. 

When learning a PDDL environment, the \texttt{World} from Algorithm~\ref{algo:science} consists of a PDDL domain as well as a state describing the current configuration of the world.

Given a PDDL problem, it is then the agent's task to reach the goal specified by the problem by learning the effects and preconditions of the relevant actions. Thus, in this instance the \textit{question} from algorithm~\ref{algo:science} is a PDDL problem, and the $canAnswer$ function consists of checking whether the problem has been solved in the given state (denoted $isSolved$).

The most basic form of knowledge relevant to the PDDL agent is statements of fact about different fluent predicates' inclusion or exclusion from actions' preconditions and effects (although other knowledge constructs are also relevant, as shown in Sections~\ref{sec:NC:Effects} and~\ref{sec:NC:Preconditions}). From this limited information, the agent can form a hypothetical domain specification based on a given strategy. For example, if a predicate $p$ have neither been proven nor disproven to be a precondition of action $A$, the pessimistic agent will include it in its hypothetical action specification for $A$, while the optimistic agent will choose to ignore it. Both strategies will fully include proven and disproven knowledge. This hypothetical domain constitutes the \texttt{Hypothesis} from Algorithm~\ref{algo:science}.

Using a hypothetical domain (\texttt{Domain}) as described above, the agent can now attempt to form a plan for reaching the goal by conventional PDDL planning mechanisms. Such a plan is analoguous to an \texttt{Experiment}: If every action in the plan succeeds when applied to the world, the hypothetical domain is sufficient for solving the problem from the given state. If, however, execution of an action in the plan yields a state different from what the hypothetical domain predicts, then the hypothesis is flawed. Thus, \emph{conducting} an experiment in PDDL means executing a plan, which yields a new state as well as a \texttt{Result} indicating whether it failed or succeeded, and --- in the first case --- which transition caused the failure. An algorithm for the PDDL specific \textit{conduct} function is presented in Algorithm~\ref{algo:PDDLConduct}. 

\begin{algorithm}
    \begin{algorithmic}
        \Function{$\textsc{conduct}$} {\texttt{Plan} $l$, \texttt{World} $w$, \texttt{Domain} $d_h$}
        \State{$s \gets w.state$}
        \State{$d \gets w.domain$}
        \ForAll{grounded actions $a \in p$}
            \State{$s' \gets apply(d, s, a)$}
            \State{$s'_{h} \gets apply(d_h, s, a)$}
            \If{$s' \neq s'_h$}
                \State{\Return{$\left( \texttt{Failure} \left(s, a, s'\right), s\right)$}}
            \EndIf%
            \State{$s \gets s'$}
        \EndFor%
            \State{\Return{$\left(\texttt{Success}, s\right)$}}
        \EndFunction%
    \end{algorithmic}
    \caption{Conducting a PDDL experiment (executing a plan)}\label{algo:PDDLConduct}
\end{algorithm}

The \texttt{Result} returned by \textit{conduct} is used by the \textit{analyse} function to obtain new knowledge (as described in the following sections), which is then merged into the knowledge base.

If the generated hypothetical domain is too restrictive for the agent to find a plan, no new knowledge can be gained, and the agent has the option to instead seek guidance from external sources. For this purpose,~\cite{Walsh2008} introduces the concept of a \textit{teacher}, which has full knowledge of the domain and can thus be queried for the solution to a specific problem. This solution is returned as a list of state transitions (a \texttt{Trace}), of which each one is analysed and the resulting knowledge is combined. In this context, the teacher can be seen as part of the \texttt{World}, as this already contains the actual domain. Thus, the $query$ function is analoguous to $inquire$ from Algorithm~\ref{algo:science}.

In algorithm~\ref{algo:PDDL}, an instantiation of the more general algorithm from~\ref{algo:science} is presented. Note that since the optimistic algorithm from~\cite{Walsh2008} always finds a plan that might fail, and the pessimistic only finds successfull plans or none at all, both can be derived from Algorithm~\ref{algo:PDDL} by applying the correct strategies and setting $update$ to the identity function.

\begin{algorithm}
    \begin{algorithmic}
        \Function{$\textsc{Learn-PDDL}$} {\texttt{Problem} $p$, \texttt{Strategy} $s$, \texttt{Knowledge} $k$, \texttt{World} $w$}
            \While{$\neg isSolved(p, w)$}
                \State{\texttt{Domain} $d \gets form(p, k, s)$}
                \State{\texttt{Plan} $l \gets design(w, d, s)$}
                \If{a plan is found}
                    \State{$(\texttt{Result} \, r, w) \gets conduct(l, w, d)$}
                    \State{$k \gets k + analyse(k, r)$}
                \Else%
                    \State{\texttt{Trace} $t \gets query(w, p)$}
                    \State{$k \gets fold \left( analyse, k, t \right)$}
                    % \State{$k \gets k + query(w, p)$}
                \EndIf%
                \State{$s \gets update(s, k)$}
            \EndWhile%
            \State{\Return{$k$}}
        \EndFunction%
    \end{algorithmic}
    \caption{Scientific learning algorithm for the PDDL framework}\label{algo:PDDL}
\end{algorithm}
\end{document}
