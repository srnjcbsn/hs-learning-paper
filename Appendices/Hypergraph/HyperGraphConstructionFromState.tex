\providecommand{\master}{../..}
\documentclass[\master/Master.tex]{subfiles}

\begin{document}
Hypergraphs is a representation of the all possible patterns that could possibly produce an effect. In this section we will show how we can reduce this pattern such that it will in end be equal to the real pattern of preconditions in the conditional.

Consider again a state transition $\left( S, a, S' \right)$. The hypergraph constructed for $\ts(S)$ (as shown in \Cref{sec:ca:hgintro}) now describes how the atoms in $S$ are interconnected without mentioning specific objects or variables. Similarly, a single effect $p$ of action $a$ can be encoded as a hypergraph by the function $hypergraph(p,\ts(S))$, which constructs a hypergraph for $\ts(S) \cup \{p\}$, labelling the atom edge describing $p$ as an effect.

Such a hypergraph describes which bindings were present in $\ts(S)$ when the effect $p$ occurred, and how $p$ is connected to them. In other words, it denotes the preconditions for the conditional that spawned $p$ which has not been disproven, and any binding \textit{not} in the hypergraph is disproven to be a precondition.

\begin{proposition}\label{prop:ca:disprovenHg}
    For a state transition $\left( S, a, S' \right)$ and a literal $p \in \Delta S$, any binding absent from $hypergraph \left( p, X(S) \right)$ is not a precondition for the conditional in $A$ with $p$ as effect. 
\end{proposition}

\begin{proof}[Proof by contradiction]
    Consider a state transition $\left( S, a, S' \right)$, a literal $p \left( o_1 \right) \in \Delta S$, and the hypergraph $H = hypergraph\left( p\left(o_1\right), X(S) \right)$. If a binding $(p_1, q_1)$ is not present in $H$ (meaning that there is no binding edge in $H$ containing $p_1$ and $q_1$), then that binding is not present in $X(S)$ (i.e.\ $q\left(o_1\right) \notin S$). Thus, if that binding was a precondition for the conditional that spawned $p\left(o_1\right)$, then that interpretation of the conditional would have failed, and $p\left( o_1 \right)$ would not have occurred in $\Delta S$. Additionally, by Proposition~\ref{prop:ca:singleConditional}, no other conditional in $A$ could have caused the effect $p \left(o_1\right)$.
\end{proof}

Recall that any two literals $p$ and $q$ in $\Delta S$ with the same name are effects of the same conditional (by Proposition~\ref{prop:ca:singleConditional}), and that any bindings absent from \textit{either} of the hypergraphs $H_p = hypergraph(p, X(S))$ and $H_q = hypergraph(q, X(S))$ are disproven to be preconditions of that conditional (by Proposition~\ref{prop:ca:disprovenHg}). Consequently, if the two hypergraphs can be merged into one hypergraph $H'$, such that $H'$ only contains bindings that $H_p$ and $H_q$ has in common, then any binding absent from $H'$ can not be a precondition for the conditional either. 

In the following, we will assume availability of such a merging function (denoted $merge$), and a detailed algorithm can be found in Section~\ref{sec:C:HGMerging}.

From the above, it follows that all grounded literals with name $n$ in $\Delta S$ can be merged together to disprove preconditions of the conditional that spawned them. The resulting hypergraph then constitutes all bindings that have not explicitly been disproven to be literals, and can as such be reused in another transition. This hypergraph describes the unproven knowledge $U$.

As with connecting paths, we have the concept of candidates, i.e.\ reasons a conditional could have failed in a certain interpretation. Recall that such a reason is a connecting path that was not present in the failing state. Since vertices in a hypergraph can be uniquely identified, and paths to the effect can be described by a binding to another vertex that is itself transitively connected, a candidate for failure can be encoded as a pair of vertices. I.e.\ it is sufficient to record the last component of the offending connecting path. Note that this pair can represent several connecting paths.

Similarly to nonconditional actions and connection paths, a candidate is proven to be a precondition if the candidate set it is part of is reduced to the singleton set. Such a reduction can be accomplished by removing pairs from a candidate set if there is no binding edge in $U$ containing both vertices of the pair. Given a set $c$ of vertex pairs and the set of binding edges of $U$, the $reduce$ function removes candidates from $c$ as described above.

\begin{equation*}
    reduce(c, E_B) =
    \left\{ \left( t_1, t_2 \right) \mid 
        \left( t_1, t_2 \right) \in c \land 
        \exists_{b \in E_B} \left[ t_1 \in b \land t_2 \in b \right] 
    \right\}
\end{equation*}

Given a candidate set $C$, the proven preconditions can now be seen as the singleton sets in $C$, i.e.\ 

\begin{equation*}
    K(C) = \left\{ 
        c \mid c \in C \land |c| = 1
    \right\}
\end{equation*}

As each vertex in a hypergraph has unique identifiers, and identifiers for two hypergraphs are not necessarily identical, the $merge$ function must also perform an identifier renaming. Thus, the $reduce$ function described above will not work with the merged $U$, as the vertex identifiers will not match, and vertices in $U$ can not be matched with those in $K$. Therefore, $merge$ should return the renaming function used, so that it can be applied to $C$ and $K$.

As described in the next section, a vertex may have been ``shadowing'' another, such that what we perceived as a single vertex in a binding edge in $U$ were in fact two. If that is the case, the renaming function will replace that vertex with two equally-named vertices with different identifiers, both part of the binding edge the original vertex was part of. 

Thus, when this renaming is applied to a candidate tuple $(u,v)$, it produces $|rn(u)| \times |rn(v)|$ tuples, which replaces $(u,v)$ in the candidate set, i.e.\ 

\begin{equation*}
    rnCands(c, rn) =
    \left\{ 
        \left( u', v' \right) \mid 
        (u, v) \in c \land u' \in r(u) \land v' \in r(v) 
    \right\}
\end{equation*}

Observe that since the proven knowledge $K(C)$ is comprised of the singleton candidates, some ``proven'' knowledge may --- in a later transition --- be degraded to regular candidates. As such, knowledge can never be said to be decidedly proven.

% As $K$ is a union of singleton candidate sets, each vertex pair $(u,v) \in K$ can be renamed as $rnCands( \{ (u,v) \}, rn)$. If the set returned is a singleton set $\left\{ \left( u, v \right) \right\}$, then $\left(u, v\right)$ is 
% The same renaming method is applied to all vertex pairs $(u,v)$ in $K$, with an added restriction: if $|rn(u)| > 1$, then 
%
% A similar method is required for the set of proven preconditions $K$, with an added restriction: If $|rnTuple( (u,v), rn)| = 1$ for a tuple $(u,v) \in K$, then the singleton set is retained in $K$. Otherwise, $rnTuple( (u,v), rn)$ is added to $C$.

% \begin{equation*}
%     rnTuple\left( (u,v), rn \right) =
%     \left\{
%         \left( u', v' \right) \mid u' \in rn(u) \land v' \in rn(v)
%     \right\}
% \end{equation*}

In \Cref{algo:CondEffLearn}, an algorithm for updating knowledge about a single conditional is provided. The \textsc{Update-Knowledge} function takes as input a predicate $n$, a state transition, a hypergraph $U$ describing the unproven knowledge, as well as a set of candidates $\Cand$. It then merges $U$ with hypergraphs for all effects in the state transition with the same name as $n$, performs renaming (as explained above), and updates the set of candidates. Note that for a single state transition, this function should be called for every predicate in $\preds$, and an unproven hypergraph as well as a set of candidates should be maintained for every predicate in $\preds$.

\begin{algorithm}
    \caption{Algorithm for learning conditional effects using hypergraphs}\label{algo:CondEffLearn}
    \begin{algorithmic}
        \Function{Update-Knowledge}{$n$, $(S,a,S')$, $U$, $\Cand$}
            \ForAll{literals $p \in \Delta s$ with name $n$}
                \State{$H_p \gets hypergraph(p, S)$}
                \State{$\left(U, r \right) \gets merge \left( U, H_p \right)$}
                \State{$\Cand \gets rnCands(\Cand, r)$}
                % \State{$C \gets \left\{ rnCands(c, rn) \mid c \in C \right\}$}
                % \State{$K \gets rnKnown(K, rn) $}
            \EndFor
            \ForAll{literals $q \notin \Delta s$ with name $n$}
                \State{$H_q \gets hypergraph(q, s)$}
                \State{$\Cand \gets \Cand \cup cands\left( H_q, U \right)$}
            \EndFor
            \State{$\Cand = \left\{ reduce\left(c, U\right) \mid c \in C \right\}$}
            % \State{Remove any singleton sets from $C$, and add them to $K$}
            \State\Return{$(U,\Cand)$}
        \EndFunction

        % \Function{$\textsc{Conditional-Effect-Learn}$} {$\left( s, a, s'\right)$, $U$, $K$, $C$}
        %     \ForAll{predicate names $n$}
        %         \State{$\left(U_n, K_n, C_n\right) = UpdateKnowledge\left(n, \Delta s, U_n, K_n, C_n \right)$}
        %     \EndFor
        %     \State\Return{$\left( U, K, C\right)$}
        % \EndFunction%
    \end{algorithmic}
\end{algorithm}

\end{document}
