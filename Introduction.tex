\documentclass[Master.tex]{subfiles}
\begin{document}


	What does it mean to learn? This is an important question to raise within the subject of artificial intelligence.
	In science we learn and discover something new every day, as such we have built frameworks for how this learning and discovery should be conducted. From this we believe that by building on already fundamental scientific methods, we may simplify how an artificial intelligence should learn.
	
	The first question we should ask ourselves is: How can we perform an experiment and know exactly what we have learned from it, 
	and how can we use this knowledge to develop further experiments highlighting another aspect of the field we are trying to learn?
	Both of these questions turns out to be very interesting when it comes to AI learning.
	
	This means that research done on science and scientific understanding is equally relevant in the context of making an agent able to learn.
	As some of these concepts are greater than we are able to cover in this thesis, we have decided to limit ourselves to just planning domains.	 
	Planning as a field of study deals with the idea that an agent knows what it is capable of, 
	and from there must be able to achieve a desired goal. 
	We wish to look at the problem from the opposite side: What if an agent knows nothing, and instead must learn through induction what it can achieve. 
	This is interesting as it parallels the experience of an infant, when it is initially born  --- ignoring genetic memory --- such an infant should have no prior knowledge, and therefore must use inductive reasoning to begin to understand how it can act upon its environment. For each action it takes, it may learn of its capabilities, but only through failures may it learn its limitations.
	
	The \cite{Walsh2008} paper provides two algorithms that shows how to inductively learn action schemas which define a planning domain. 
	Each algorithm describes a unique strategy for an agent to learn based on. 
	
	The first (which they label as ``optimistic''), is an approach where the agent always assumes that it is capable of more than it has seen it is. This makes it so, initially when the agent has no knowledge of anything it is quickly able to find plan, now that plan will almost always quickly fail however just by trying to perform the plan the agent is able to learn something new which it can use next time it tries to find a plan. Thus through trial and error the agent is slowly able to learn the domain.
	
	The second algorithm (labelled ``pessimistic'') is in stark contrast to the first. With this approach, the agent assumse it is capable of nothing, unless proven otherwise. This means the agent must rely on a teacher to explain how goals are solved, and instead the task for the agent is to learn from the directions given from an outside source, eventually gaining enough knowledge to autonomously complete tasks.
	
    In this thesis, we will use~\cite{Walsh2008} and the optimistic and pessimistic algorithms as a point of reference to explore the subject. The paper deals exclusively with set-theoretic planning actions, in a STRIPS-style domain, where they have excluded disjunctive and negative preconditions. 
	We seek to expand on this, showing how learning negative preconditions can be achieved. Furthermore, we will also study, and propose a preliminary solution to the problem of learning actions with universally quantified conditional effects. That is, actions that are able to influence many different objects in the universe, and whose effects may occur independently of each other.
	
    These problems often depends on minute details, but they have an impact on how learning should be handled. For this reason we will be using a running example throughout the thesis such that problems are easily made apparent. The running example we will be using is that of the 1981 video game Sokoban, where a worker agent is tasked with moving crates onto specific positions in a tightly packed warehouse\footnote{See e.g.\ \url{https://en.wikipedia.org/wiki/Sokoban}}.
	
	Throughout this thesis we will solve several issues pertaining to different aspects of learning. The following list will give an overview of what we have looked into and discussed:
	\begin{description}
		\item[Negative Preconditions] 
		By define a new approach to learning and understanding preconditions, we are able to handle positive as well as negative preconditions.
		
		\item[Negative Goals] 
		One of the limitations of \cite{Walsh2008} optimistic algorithm was it is incapable of solving Negative goals, we have solved this issue by designing a new theoretical planner feature.
		
		\item[Generalized Learning algorithm] 
		We discuss how and algorithm that constitutes learning should be defined, and we formulate one based on the theory of scientific learning.
		
		\item[Preliminary Conditional Effect Learning]
		The reason we say preliminary is because, while we have developed and define a very efficient model for describing conditional effects, and shown how it should be used. We have not been able to design an efficient merge algorithm between two of our models. And while we have identified the problem of merging, we have had insufficient time to fully explore it.
		
		\item[Tested the theory by implementing it]
		As we eluded to with conditional actions we have not fully explored this problem, but we have explored the problem fully for non-conditional actions. For this reason we have chosen to design a fully working program for non-conditional actions. 
		This program includes a sokoban environment  where an agent must learn what its action does, in order to solve a set of goals.
		
	\end{description}
	
 \subsection*{Roadmap}
 
 \begin{description}
 	\item[\nameref{sec:Learning}]
	 	This chapter gives an simplified view of scientific learning, how strategies in learning and what differentiate them.  
	\item[\nameref{sec:Algorithm}]
		This section provides an algorithm for learning, and shows how it can be applied to a PDDL action schema learning.
	\item[\nameref{chp:nca}]
		Shows how STRIPs-style action schema learning can be achieved. It shows how to learn both the preconditions and the effects of such an action. Lastly it shows how different learning strategies can be used when constructing the action schema for the action.
		
	\item[\nameref{chp:ca}]
		This chapter provides a detailed description of the problems when attempting to learn conditional effects, and provides several different models that improves understanding of the problem. Especially we would like to highlight our optimized Hypergraph model as it provides a mathematical framework for efficient and explicit description of the problem of conditonal effects .
	
	\item[\nameref{chp:imp}]
		Shows the results from our simulation runs, and provides a detailed description of what we have design.
 
\end{description}
	
%%%%%%%%%%%%%%%%%%%%%%%%%
% Overview
%%%%%%%%%%%%%%%%%%%%%%%%%

% Introduction:
% 	- Introduce problem of learning (motivation)
% 	- Context: Walsh (explain optimistic and pessimistic algos), Conditional effects (abstract introduction)
% 	- Problem restatement: Sokoban
% 	- Result restatement: Conditional effects, better precondition learning (negative preconditions), more general algorithm
% 		* Limits: No disjunctive preconds, (no parameters)
% 	- Road map
%
% Introduction to Learning
% 	- Explain scientific learning, how to learn scientifically?
% 		* Strategy
% 	- Algorithm
% 	- Strategy examples
% 	- framework for worlds (pddl)
% 	- Sokoban as pddl
% 	- Algorithm for pddl
%
% Nonconditional(Strips-style) Learning
% 	- Non-conditional actions in pddl
% 	- Sokoban example
% 	- Effect Learning
% 	- Precondition Learning
% 	- Hypothesis construction
% 		* How to: optimistic algorithm
% 		* How to: pessimistic algorithm
%
% Conditional effects learning
% 	- What is conditional effects
% 	- Difficulities learning conditional effects
% 	- Explain our models
% 	- Hyper graph construction from state
% 	- Hyper graph merging algorithm
% 	- Hypothesis construction
% 		* Pessimistic strategy
% 		* Problems with using optimistic strategy
% 		* Algorithm for hypothesis from hyper graph
%
% Methods (Implementation)
% 	- Parsing domains
% 	- Problems with fast-downward
% 		* Preprocessing (SAS+)
% 		* A* implementation
% 	- bounded planner optimization
% 	- Nonconditional optimistic
% 	- Conditional pessimistic
%
% Results
%
% Discussion
% 	- Comparison to Walsh
% 	- Limitations
% 	- Expressive power
%
% Conclusion
% 	- more general algorithm
% 	- Conditional effects

\end{document}
