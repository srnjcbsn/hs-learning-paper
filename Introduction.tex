\documentclass[Master.tex]{subfiles}
\begin{document}


	Within the field of AI most studies are based on 
	machine learning and statistical analysis. And while those are good approaches
	to achieve a learning agent, it does not broaden our understanding of what it means to learn.
	What we mean by this is: How can we perform an experiment and know exactly what we have learned from it, 
	and how we can use this knowledge to develop further experiments highlighting another aspect of the domain we are trying to learn.
	Both of these questions turns out to be very interesting when it comes to AI learning.
	In reality AI learning is no different from scientific learning, as we are all agents occupying the current universe we exist in.
	This means that research done on science and scientific understanding is equally relevant in the context of making an agent able to learn.
	As some of these concepts are greater than we are able to cover in this thesis, we have decided to limit ourselves to just planning domains.	 
	Planning as a field of study deals with the idea that an agent knows what it is capable of, 
	and from there must be able to achieve a desired goal. 
	We wish to look at the problem from the opposite side, what if an agent knows nothing it is capable of and instead must learn through induction what it can achieve. This parallels the experience of an infant, when it initially is brought into the world. Ignoring genetic memory such an infant should have no prior knowledge, and therefore must use inductive reasoning to begin to understand how it can act upon its environment. For each action it takes the more knowledge it will have of its capabilities, however only through failures it may learn its limitations this is a provable fact.
	 
	
	
	The \cite{Walsh2008} paper provides two algorithms that shows how to inductively learn action schemas which define a planning domain. 
	Each algorithm describes a unique strategy for an agent to learn based on. 
	
	The first which they dub optimistic, is an approach where the agent always assumes it capable of more than it has seen it is. This makes it so, initially when the agent has no knowledge of anything it is quickly able to find plan, now that plan will almost always quickly fail however just by trying to perform the plan the agent is able to learn something new which it can use next time it tries to find a plan. Thus through trail and error the agent is slowly able to learn the domain.
	
	The second algorithm in stark contrast to the first is called pessimistic, this algorithm is based around making the agent assume it is capable of nothing unless proven otherwise. This means the agent must rely on a teacher to explain how goals are solved, and instead the task for agent is how to understand the directions given from an outside source such as a teacher.
	
	This paper and these two approaches we use as a point of reference for us to explore the subject.  The paper deals exclusively with set-theoretic planning actions, in a STRIPS-style domain, where they have excluded disjunctive preconditions and negative preconditions. 
	We seek to expand upon this showing how learning of negative precondition learning can be achieved. Furthermore, we will also show how conditional action learning may be achieved. By conditional actions we mean actions that include effects that sometime occur dependent on the state the action is applied in.
	
	
	To given an example of one of problems with learning STRIPS-style actions is that. For instance, predicates that are outputted by an action often cannot be reversed to its original fluent predicate. Case in point, an Action $A(x,y)$ applied with the arguments $A(o_1,o_1)$, that outputs a predicate $p(o_1,o_2)$, creates a dilemma for the learning agent as it cannot deduce what the predicate that spawned it was as $p(x,x), p(x,y), p(y,x), p(y,y)$ are all viable candidates. An extension of this problem is where the agent receives different pieces of knowledge for a given predicate and must by piecing the knowledge together be able to determine which possible predicate is the correct one.
	
	As evident these problems often depends on minute details, but they have a huge impact on how learning should be handled. For this reason we will be using a running example throughout the thesis such that problems are easily made apparent. The running example we will be using is that of the $1981$ video game sokoban, that is about a person trying to move creates onto specific positions in a tightly packed warehouse.
	
	Throughout this thesis we will solve many different issues pertaining to all sort of different aspects of learning,
	the following list will give an overview of what we have looked into and discussed:
	\begin{description}
		\item[Negative\; Preconditions] 
		By define a new approach to learning and understanding preconditions, we are able to handle positive as well as negative preconditions.
		
		\item[Negative\; Goals] 
		One of the limitations of \cite{Walsh2008} optimistic algorithm was it is incapable of solving Negative goals, we have solved this issue by designing a new theoretical planner feature.
		
		\item[Generalized\; Learning\; algorithm] 
		We discuss how and algorithm that constitutes learning should be defined, and we formulate one based on the theory of scientific learning.
		
		\item[Preliminary\;Conditional\;Effect\;Learning]
		The reason we say preliminary is because, while we have developed and define a very efficient model for describing conditional effects, and shown how it should be used. We have not been able to design an efficient merge algorithm between two of our models. And while we have identified the problem of merging, we have had insufficient time to fully explore it.
		
		\item[Tested \; the \; theory \; by \; implementing \; it]
		As we eluded to with conditional actions we have not fully explored this problem, but we have explored the problem fully for non-conditional actions. For this reason we have chosen to design a fully working program for non-conditional actions. 
		This program includes a sokoban environment  where an agent must learn what its action does, in order to solve a set of goals.
		
	\end{description}
	
 \subsection*{Roadmap}
 
 \begin{description}
 	\item[\nameref{sec:Learning}]
	 	This section gives an simplified view of scientific learning, how strategies in learning and what differentiate them. It also provides definition of how we use PDDL in rest of the thesis and provides a sokoban model in PDDL.
	 	Lastly this section also provides an algorithm for learning and 
	
	\item[\nameref{chp:nca}]
	
	\item[\nameref{chp:ca}]
	
	\item[\nameref{chp:imp}]
	
	\item[\nameref{chp:res}]
	
	\item[\nameref{chp:dis}]
 
\end{description}
	
%%%%%%%%%%%%%%%%%%%%%%%%%
% Overview
%%%%%%%%%%%%%%%%%%%%%%%%%

% Introduction:
% 	- Introduce problem of learning (motivation)
% 	- Context: Walsh (explain optimistic and pessimistic algos), Conditional effects (abstract introduction)
% 	- Problem restatement: Sokoban
% 	- Result restatement: Conditional effects, better precondition learning (negative preconditions), more general algorithm
% 		* Limits: No disjunctive preconds, (no parameters)
% 	- Road map
%
% Introduction to Learning
% 	- Explain scientific learning, how to learn scientifically?
% 		* Strategy
% 	- Algorithm
% 	- Strategy examples
% 	- framework for worlds (pddl)
% 	- Sokoban as pddl
% 	- Algorithm for pddl
%
% Nonconditional(Strips-style) Learning
% 	- Non-conditional actions in pddl
% 	- Sokoban example
% 	- Effect Learning
% 	- Precondition Learning
% 	- Hypothesis construction
% 		* How to: optimistic algorithm
% 		* How to: pessimistic algorithm
%
% Conditional effects learning
% 	- What is conditional effects
% 	- Difficulities learning conditional effects
% 	- Explain our models
% 	- Hyper graph construction from state
% 	- Hyper graph merging algorithm
% 	- Hypothesis construction
% 		* Pessimistic strategy
% 		* Problems with using optimistic strategy
% 		* Algorithm for hypothesis from hyper graph
%
% Methods (Implementation)
% 	- Parsing domains
% 	- Problems with fast-downward
% 		* Preprocessing (SAS+)
% 		* A* implementation
% 	- bounded planner optimization
% 	- Nonconditional optimistic
% 	- Conditional pessimistic
%
% Results
%
% Discussion
% 	- Comparison to Walsh
% 	- Limitations
% 	- Expressive power
%
% Conclusion
% 	- more general algorithm
% 	- Conditional effects

\end{document}
